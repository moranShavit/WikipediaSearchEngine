{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security → Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7919fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff26758",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'ir_3_207472234' \n",
    "relative_path = f\"meta_data\"\n",
    "client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = \"gs://ir_3_207472234/multistream*_preprocessed.parquet\"\n",
    "\n",
    "parquetFile = spark.read.parquet(paths)\n",
    "\n",
    "doc_ids_rdd = parquetFile.select( \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e689243",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c8046",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7422efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text_rdd = parquetFile.select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d99a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverted_index_gcp import InvertedIndex as idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def ids_df_stats_and_sorted(doc_ids_df):\n",
    "    \"\"\"\n",
    "    doc_ids_df: Spark DataFrame with a column 'id' (any numeric type)\n",
    "\n",
    "    Returns:\n",
    "      ids_sorted_np: numpy array sorted ascending (uint32/uint64)\n",
    "      N            : distinct doc count\n",
    "      max_id       : max doc id\n",
    "      min_id       : min doc id\n",
    "    \"\"\"\n",
    "    # Make sure we only work with one column and correct type\n",
    "    ids_df = doc_ids_df.select(F.col(\"id\").cast(\"long\").alias(\"id\")).filter(F.col(\"id\").isNotNull())\n",
    "\n",
    "    # Distinct doc ids (Spark does this distributed)\n",
    "    ids_distinct_df = ids_df.dropDuplicates([\"id\"]).cache()\n",
    "\n",
    "    # Stats in one job (Spark aggregate)\n",
    "    row = ids_distinct_df.agg(\n",
    "        F.count(\"*\").alias(\"N\"),\n",
    "        F.max(\"id\").alias(\"max_id\"),\n",
    "        F.min(\"id\").alias(\"min_id\"),\n",
    "    ).collect()[0]\n",
    "\n",
    "    N = int(row[\"N\"])\n",
    "    max_id = int(row[\"max_id\"])\n",
    "    min_id = int(row[\"min_id\"])\n",
    "\n",
    "    # Collect sorted ids to driver (Spark sorts distributed, then we collect)\n",
    "    # Note: .toPandas() can be memory-heavy; .collect() is fine for 6M on a big VM.\n",
    "    ids_sorted_list = [r[\"id\"] for r in ids_distinct_df.orderBy(\"id\").select(\"id\").collect()]\n",
    "\n",
    "    # Choose smallest dtype that fits\n",
    "    if max_id <= np.iinfo(np.uint32).max:\n",
    "        ids_sorted_np = np.array(ids_sorted_list, dtype=np.uint32)\n",
    "    else:\n",
    "        ids_sorted_np = np.array(ids_sorted_list, dtype=np.uint64)\n",
    "\n",
    "    return ids_sorted_np, N, max_id, min_id\n",
    "\n",
    "def ids_rdd_stats_and_sorted(doc_ids_rdd):\n",
    "    # doc_ids_rdd can be RDD[int] or RDD[Row] – handle both:\n",
    "    ids_only = doc_ids_rdd.map(lambda x: x.id if hasattr(x, \"id\") else x) \\\n",
    "                          .filter(lambda x: x is not None) \\\n",
    "                          .map(lambda x: int(x)) \\\n",
    "                          .distinct() \\\n",
    "                          .cache()\n",
    "\n",
    "    N = ids_only.count()\n",
    "    min_id = ids_only.min()\n",
    "    max_id = ids_only.max()\n",
    "\n",
    "    ids_sorted_np = np.array(ids_only.sortBy(lambda x: x).collect(), dtype=np.int64)\n",
    "    return ids_sorted_np, N, max_id, min_id\n",
    "\n",
    "def build_docid_to_pos(ids_sorted_np, max_id):\n",
    "    \"\"\"\n",
    "    ids_sorted_np: sorted numpy array of distinct doc ids\n",
    "    max_id: maximum doc id (int)\n",
    "\n",
    "    Returns:\n",
    "      docid_to_pos: int32 numpy array of length max_id+1 filled with -1;\n",
    "                   docid_to_pos[doc_id] = position in ids_sorted_np\n",
    "    \"\"\"\n",
    "    docid_to_pos = np.full((max_id + 1,), -1, dtype=np.int32)\n",
    "    docid_to_pos[ids_sorted_np.astype(np.int64)] = np.arange(ids_sorted_np.shape[0], dtype=np.int32)\n",
    "    return docid_to_pos\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "def save_numpy_to_gcs(arr: np.ndarray, filename: str, bucket_name: str, relative_path: str):\n",
    "    \"\"\"\n",
    "    Saves a numpy array to GCS as .npy\n",
    "\n",
    "    arr            : numpy array\n",
    "    filename       : e.g. 'docid_to_pos.npy'\n",
    "    bucket_name    : GCS bucket name (no gs://)\n",
    "    relative_path  : path inside bucket, e.g. 'metadata/body'\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        local_path = os.path.join(tmpdir, filename)\n",
    "        np.save(local_path, arr)\n",
    "\n",
    "        gcs_path = f\"gs://{bucket_name}/{relative_path}/{filename}\"\n",
    "        subprocess.check_call([\"gsutil\", \"cp\", local_path, gcs_path])\n",
    "\n",
    "        print(f\"Saved {filename} → {gcs_path}\")\n",
    "\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_sorted_np, N, max_id, min_id = ids_rdd_stats_and_sorted(doc_ids_rdd)\n",
    "docid_to_pos = build_docid_to_pos(ids_sorted_np, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = ids_sorted_np.shape[0]\n",
    "\n",
    "index = idx.InvertedIndex.read_index(\n",
    "    base_dir=f\"indexes/postings_gcp\",\n",
    "    name=\"index\",\n",
    "    bucket_name=BUCKET_NAME\n",
    ")\n",
    "\n",
    "# N must be total number of docs in corpus (distinct)\n",
    "# If you already computed it earlier, reuse it. Example:\n",
    "# N = doc_ids_df.dropDuplicates([\"id\"]).count()\n",
    "# Otherwise set N explicitly.\n",
    "N = int(N)\n",
    "\n",
    "bc_df = sc.broadcast(index.df)   # dict term -> df\n",
    "bc_N  = sc.broadcast(N)\n",
    "bc_stop = sc.broadcast(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_invlen_and_norm(text: str, doc_id: int):\n",
    "    import builtins\n",
    "    import math\n",
    "    \"\"\"\n",
    "    Returns TWO records via flatMap:\n",
    "      (\"inv_len\", (doc_id, inv_doc_len))\n",
    "      (\"norm\",    (doc_id, doc_norm))\n",
    "\n",
    "    doc_norm uses:\n",
    "      tf_norm = tf / doc_len   (implemented as tf * inv_doc_len)\n",
    "      idf = log(N / df)\n",
    "      norm = sqrt( sum_t (tf_norm * idf)^2 )\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "\n",
    "    # tokenize\n",
    "    tokens = [m.group() for m in RE_WORD.finditer(text.lower())]\n",
    "\n",
    "    # term freq after stopwords\n",
    "    stop = bc_stop.value\n",
    "    tf = {}\n",
    "    for t in tokens:\n",
    "        if t in stop:\n",
    "            continue\n",
    "        tf[t] = tf.get(t, 0) + 1\n",
    "\n",
    "    # doc_len = total kept tokens\n",
    "    doc_len = builtins.sum(tf.values())\n",
    "    if doc_len == 0:\n",
    "        inv_len = 0.0\n",
    "        norm = 0.0\n",
    "        return [(\"inv_len\", (int(doc_id), float(inv_len))),\n",
    "                (\"norm\",    (int(doc_id), float(norm)))]\n",
    "\n",
    "    inv_len = 1.0 / float(doc_len)\n",
    "\n",
    "    # norm accumulation\n",
    "    df_dict = bc_df.value\n",
    "    N_docs = float(bc_N.value)\n",
    "\n",
    "    acc = 0.0\n",
    "    for term, f in tf.items():\n",
    "        df_t = df_dict.get(term, 0)\n",
    "        if df_t <= 0:\n",
    "            continue\n",
    "        idf = math.log(N_docs / float(df_t))\n",
    "        w = (float(f) * inv_len) * idf\n",
    "        acc += w * w\n",
    "\n",
    "    norm = math.sqrt(acc)\n",
    "\n",
    "    return [(\"inv_len\", (int(doc_id), float(inv_len))),\n",
    "            (\"norm\",    (int(doc_id), float(norm)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05086ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run over doc_text_rdd and split into two RDDs\n",
    "# doc_text_rdd is (text, id) \n",
    "tagged = doc_text_rdd.flatMap(lambda x: doc_invlen_and_norm(x[0], x[1]))\n",
    "\n",
    "doc_to_len  = tagged.filter(lambda x: x[0] == \"inv_len\").map(lambda x: x[1])  # (doc_id, inv_len)\n",
    "doc_to_norm = tagged.filter(lambda x: x[0] == \"norm\").map(lambda x: x[1])     # (doc_id, norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inv_doc_len = np.zeros(N, dtype=np.float32)\n",
    "doc_norm    = np.zeros(N, dtype=np.float32)\n",
    "\n",
    "\n",
    "for doc_id, inv_len in doc_to_len.toLocalIterator():\n",
    "    pos = docid_to_pos[doc_id]\n",
    "    if pos != -1:\n",
    "        inv_doc_len[pos] = inv_len\n",
    "\n",
    "\n",
    "for doc_id, norm in doc_to_norm.toLocalIterator():\n",
    "    pos = docid_to_pos[doc_id]\n",
    "    if pos != -1:\n",
    "        doc_norm[pos] = norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_id_rdd = parquetFile.select(\"title\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_titles_array_from_rdd(title_id_rdd, docid_to_pos: np.ndarray, N: int, max_id: int):\n",
    "#     \"\"\"\n",
    "#     title_id_rdd: RDD of (title, id) OR (id, title)\n",
    "#     docid_to_pos: numpy int32 array, length max_id+1, maps doc_id -> pos (or -1)\n",
    "#     N: number of docs (len(ids_sorted_np))\n",
    "#     max_id: maximum doc_id (for safety checks)\n",
    "\n",
    "#     Returns:\n",
    "#       titles_by_pos: numpy object array of length N, where titles_by_pos[pos] = title\n",
    "#     \"\"\"\n",
    "#     titles_by_pos = np.empty(N, dtype=object)\n",
    "#     titles_by_pos[:] = \"\"\n",
    "\n",
    "#     def normalize(x):\n",
    "#         a, b = x\n",
    "#         if isinstance(a, (int, np.integer)) and isinstance(b, str):\n",
    "#             return int(a), b  # (id, title)\n",
    "#         if isinstance(b, (int, np.integer)) and isinstance(a, str):\n",
    "#             return int(b), a  # (id, title)\n",
    "#         # fallback: assume (title, id)\n",
    "#         return int(b), str(a)\n",
    "\n",
    "#     # Stream results to driver (no giant collect)\n",
    "#     for did, title in title_id_rdd.map(normalize).toLocalIterator():\n",
    "#         if 0 <= did <= max_id:\n",
    "#             pos = docid_to_pos[did]\n",
    "#             if pos != -1:\n",
    "#                 titles_by_pos[pos] = title if title is not None else \"\"\n",
    "\n",
    "#     return titles_by_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles_by_pos = build_titles_array_from_rdd(title_id_rdd, docid_to_pos, N, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7358d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert inv_doc_len.dtype == np.float32\n",
    "assert doc_norm.dtype == np.float32\n",
    "assert inv_doc_len.shape == doc_norm.shape == (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deeba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_numpy_to_gcs(\n",
    "    docid_to_pos,\n",
    "    filename=\"doc_id_to_pos.npy\",\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    relative_path=relative_path\n",
    ")\n",
    "\n",
    "save_numpy_to_gcs(\n",
    "    ids_sorted_np,\n",
    "    filename=\"sorted_doc_ids.npy\",\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    relative_path=relative_path\n",
    ")\n",
    "\n",
    "save_numpy_to_gcs(\n",
    "    inv_doc_len,\n",
    "    filename=\"inv_doc_len_body.npy\",\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    relative_path=relative_path\n",
    ")\n",
    "\n",
    "save_numpy_to_gcs(\n",
    "    doc_norm,\n",
    "    filename=\"doc_norm_body.npy\",\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    relative_path=relative_path\n",
    ")\n",
    "\n",
    "# save_numpy_to_gcs(\n",
    "#     arr=titles_by_pos,\n",
    "#     filename=\"doc_id_to_title.npy\",\n",
    "#     bucket_name=BUCKET_NAME,\n",
    "#     relative_path=relative_path\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
