{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f37154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security â†’ Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4381ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'ir_3_207472234' \n",
    "relative_path = f\"meta_data\"\n",
    "client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = \"gs://ir_3_207472234/multistream*_preprocessed.parquet\"\n",
    "\n",
    "parquetFile = spark.read.parquet(paths)\n",
    "\n",
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_title_pairs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import builtins\n",
    "import numpy as np\n",
    "import os, struct\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "GCS_BASE = \"gs://ir_3_207472234/metadata\" \n",
    "ID2POS_GCS = \"gs://ir_3_207472234/meta_data/doc_id_to_pos.npy\"\n",
    "\n",
    "NUM_PARTS = 256  # tune: 256-2048 depending on cluster\n",
    "\n",
    "\n",
    "# Make doc_id_to_pos.npy available on every executor\n",
    "sc.addFile(ID2POS_GCS)\n",
    "\n",
    "# -----------------------------\n",
    "# Map to (pos, title_bytes)\n",
    "# - load id_to_pos once per executor JVM via lazy global\n",
    "# -----------------------------\n",
    "_id2pos = None\n",
    "\n",
    "def get_id2pos():\n",
    "    global _id2pos\n",
    "    if _id2pos is None:\n",
    "        path = SparkFiles.get(\"doc_id_to_pos.npy\")\n",
    "        # mmap = low memory, fast enough; file is local on executor\n",
    "        _id2pos = np.load(path, mmap_mode=\"r\")\n",
    "    return _id2pos\n",
    "\n",
    "id2pos = get_id2pos()\n",
    "INVALID_POS = np.uint32(2**32 - 1) if id2pos.dtype == np.uint32 else -1\n",
    "\n",
    "def to_pos_and_bytes(row):\n",
    "    # row is pyspark.sql.Row(title=..., id=...)\n",
    "    doc_id = int(row[\"id\"])\n",
    "    title = row[\"title\"]\n",
    "    if title is None:\n",
    "        title = \"\"  # keep empty titles\n",
    "\n",
    "    id2pos = get_id2pos()\n",
    "    pos = id2pos[doc_id]\n",
    "\n",
    "    # skip missing doc_id if your mapping uses sentinel\n",
    "    if pos == INVALID_POS:\n",
    "        return None\n",
    "\n",
    "    b = title.encode(\"utf-8\")\n",
    "    return (int(pos), b)\n",
    "\n",
    "pos_bytes = (\n",
    "    doc_title_pairs\n",
    "    .map(to_pos_and_bytes)\n",
    "    .filter(lambda x: x is not None)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure order by pos (required for contiguous blob indexing)\n",
    "# Use repartitionAndSortWithinPartitions for scalable sort\n",
    "# -----------------------------\n",
    "from pyspark.rdd import portable_hash\n",
    "\n",
    "\n",
    "sorted_rdd = pos_bytes.sortBy(lambda x: x[0], numPartitions=NUM_PARTS)\n",
    "\n",
    "# Materialize cache\n",
    "sorted_rdd = sorted_rdd.persist(StorageLevel.DISK_ONLY)\n",
    "_ = sorted_rdd.count()\n",
    "\n",
    "# -----------------------------\n",
    "# Pass A: partition summaries (byte totals + doc counts)\n",
    "# -----------------------------\n",
    "def part_summary(iterable):\n",
    "    n = 0\n",
    "    total_bytes = 0\n",
    "    last_pos = None\n",
    "    for pos, b in iterable:\n",
    "        n += 1\n",
    "        total_bytes += len(b)\n",
    "        last_pos = pos\n",
    "    # return (n_docs, total_bytes, last_pos) for sanity checks\n",
    "    yield (n, total_bytes, last_pos if last_pos is not None else -1)\n",
    "\n",
    "summaries = sorted_rdd.mapPartitions(part_summary).collect()\n",
    "\n",
    "part_doc_counts = [s[0] for s in summaries]\n",
    "part_byte_counts = [s[1] for s in summaries]\n",
    "\n",
    "# prefix sums for base byte offset per partition\n",
    "base_offsets = [0] * len(part_byte_counts)\n",
    "running = 0\n",
    "for i, bc in enumerate(part_byte_counts):\n",
    "    base_offsets[i] = running\n",
    "    running += bc\n",
    "\n",
    "total_docs = builtins.sum(part_doc_counts)\n",
    "total_bytes = running\n",
    "\n",
    "print(\"total_docs:\", total_docs, \"total_bytes:\", total_bytes)\n",
    "\n",
    "base_offsets_bc = sc.broadcast(base_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bd0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir -p gs://ir_3_207472234/metadata/tmp_titles_data\n",
    "!hadoop fs -mkdir -p gs://ir_3_207472234/metadata/tmp_titles_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls gs://ir_3_207472234/metadata/ | head\n",
    "!hadoop fs -ls gs://ir_3_207472234/metadata/tmp_titles_data | head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Pass B: write binary shards\n",
    "# We'll write to local executor disk then use Hadoop FS to copy to GCS\n",
    "# (works in Dataproc + most Spark-on-GCP setups)\n",
    "# -----------------------------\n",
    "from py4j.java_gateway import java_import\n",
    "\n",
    "\n",
    "\n",
    "end_off_path = \"/tmp/end_off.bin\"\n",
    "with open(end_off_path, \"wb\") as f:\n",
    "    f.write(struct.pack(\"<Q\", total_bytes))\n",
    "\n",
    "import os, struct, subprocess\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_shards(part_idx, iterable):\n",
    "    base = base_offsets_bc.value[part_idx]\n",
    "    cur = base\n",
    "\n",
    "    tmp_dir = f\"/tmp/p5_titles_{part_idx:05d}\"\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "    data_path = os.path.join(tmp_dir, f\"titles_data_part_{part_idx:05d}.bin\")\n",
    "    off_path  = os.path.join(tmp_dir, f\"titles_offsets_part_{part_idx:05d}.bin\")\n",
    "\n",
    "    with open(data_path, \"wb\") as fdata, open(off_path, \"wb\") as foff:\n",
    "        for pos, b in iterable:\n",
    "            foff.write(struct.pack(\"<Q\", cur))\n",
    "            fdata.write(b)\n",
    "            cur += len(b)\n",
    "            \n",
    "    gcs_data_dir = f\"{GCS_BASE}/tmp_titles_data\"\n",
    "    gcs_off_dir  = f\"{GCS_BASE}/tmp_titles_offsets\"\n",
    "    \n",
    "    subprocess.check_call([\n",
    "        \"hadoop\", \"fs\", \"-copyFromLocal\", \"-f\",\n",
    "        data_path,\n",
    "        f\"{gcs_data_dir}/part-{part_idx:05d}.bin\"\n",
    "    ])\n",
    "\n",
    "    subprocess.check_call([\n",
    "        \"hadoop\", \"fs\", \"-copyFromLocal\", \"-f\",\n",
    "        off_path,\n",
    "        f\"{gcs_off_dir}/part-{part_idx:05d}.bin\"\n",
    "    ])\n",
    "    yield (part_idx, cur)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_offsets = sorted_rdd.mapPartitionsWithIndex(write_shards).collect()\n",
    "end_offsets.sort()\n",
    "\n",
    "for part_idx, cur_end in end_offsets:\n",
    "    expected_end = base_offsets[part_idx] + part_byte_counts[part_idx]\n",
    "    if cur_end != expected_end:\n",
    "        print(\"Mismatch at part\", part_idx, \"cur_end\", cur_end, \"expected\", expected_end)\n",
    "        break\n",
    "else:\n",
    "    print(\"All partition byte counts match.\")\n",
    "\n",
    "\n",
    "print(\"done writing shards\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls gs://ir_3_207472234/metadata/tmp_titles_data | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"gs://ir_3_207472234\"\n",
    "GCS_BASE = f\"{BUCKET}/metadata\"\n",
    "\n",
    "FINAL_DATA = f\"{GCS_BASE}/titles_data.bin\"\n",
    "FINAL_OFFS = f\"{GCS_BASE}/titles_offsets.bin\"\n",
    "\n",
    "TMP_DATA_DIR = f\"{GCS_BASE}/tmp_titles_data\"\n",
    "TMP_OFFS_DIR = f\"{GCS_BASE}/tmp_titles_offsets\"\n",
    "TMP_END_OFF  = f\"{TMP_OFFS_DIR}/_end_off.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd30f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://ir_3_207472234/metadata/tmp_titles_data/part-*.bin | sort > /tmp/data_parts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/data_batches && mkdir -p /tmp/data_batches\n",
    "!split -l 32 /tmp/data_parts.txt /tmp/data_batches/batch_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eadf478",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# compose each batch -> intermediate object\n",
    "i=0\n",
    "for f in /tmp/data_batches/batch_*; do\n",
    "  out=\"gs://ir_3_207472234/metadata/_tmp_titles_data_inter_${i}.bin\"\n",
    "  gsutil compose $(cat \"$f\") \"$out\"\n",
    "  i=$((i+1))\n",
    "don"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://ir_3_207472234/metadata/_tmp_titles_data_inter_*.bin | sort > /tmp/data_inter.txt\n",
    "!gsutil compose $(cat /tmp/data_inter.txt) gs://ir_3_207472234/metadata/titles_data.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a888e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil rm {TMP_DATA_DIR}/part-*.bin || true\n",
    "!gsutil rm {TMP_DATA_INTER}*.bin || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://ir_3_207472234/metadata/tmp_titles_offsets/part-*.bin | sort > /tmp/offs_parts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af05cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp /tmp/end_off.bin {TMP_END_OFF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/tmp/offs_parts.txt\", \"a\") as f:\n",
    "    f.write(TMP_END_OFF + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/offs_batches && mkdir -p /tmp/offs_batches\n",
    "!split -l 32 /tmp/offs_parts.txt /tmp/offs_batches/batch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5440b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "i=0\n",
    "for f in /tmp/offs_batches/batch_*; do\n",
    "  out=\"gs://ir_3_207472234/metadata/_tmp_titles_offsets_inter_${i}.bin\"\n",
    "  gsutil compose $(cat \"$f\") \"$out\"\n",
    "  i=$((i+1))\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://ir_3_207472234/metadata/_tmp_titles_offsets_inter_*.bin | sort > /tmp/offs_inter.txt\n",
    "!gsutil compose $(cat /tmp/offs_inter.txt) gs://ir_3_207472234/metadata/titles_offsets.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /tmp/offs_inter.txt\n",
    "!tail -3 /tmp/offs_inter.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7803ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil rm gs://ir_3_207472234/metadata/tmp_titles_data/part-*.bin || true\n",
    "!gsutil rm gs://ir_3_207472234/metadata/tmp_titles_offsets/part-*.bin || true\n",
    "\n",
    "!gsutil rm gs://ir_3_207472234/metadata/_tmp_titles_data_inter_*.bin || true\n",
    "!gsutil rm gs://ir_3_207472234/metadata/_tmp_titles_offsets_inter_*.bin || true\n",
    "\n",
    "!gsutil rm gs://ir_3_207472234/metadata/_tmp_titles_offsets_end.bin || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed90a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -l gs://ir_3_207472234/metadata/titles_data.bin\n",
    "!gsutil ls -l gs://ir_3_207472234/metadata/titles_offsets.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b737253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, subprocess, tempfile\n",
    "\n",
    "FINAL_DATA = \"gs://ir_3_207472234/metadata/titles_data.bin\"\n",
    "FINAL_OFFS = \"gs://ir_3_207472234/metadata/titles_offsets.bin\"\n",
    "\n",
    "\n",
    "tmpdir = tempfile.mkdtemp()\n",
    "local_offs = os.path.join(tmpdir, \"titles_offsets.bin\")\n",
    "\n",
    "# download offsets (not huge)\n",
    "subprocess.check_call([\"gsutil\", \"cp\", FINAL_OFFS, local_offs])\n",
    "\n",
    "offs = np.memmap(local_offs, dtype=np.uint64, mode=\"r\")\n",
    "print(\"offsets_len:\", len(offs))\n",
    "print(\"last_offset:\", int(offs[-1]))\n",
    "\n",
    "# get data length without downloading it\n",
    "lines = subprocess.check_output([\"gsutil\", \"ls\", \"-l\", FINAL_DATA], text=True).strip().splitlines()\n",
    "first = [ln for ln in lines if not ln.startswith(\"TOTAL:\")][0]\n",
    "data_len = int(first.split()[0])\n",
    "print(\"data_len:\", data_len)\n",
    "\n",
    "assert int(offs[-1]) == data_len\n",
    "print(\"OK: offsets[-1] matches titles_data length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaac089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use example\n",
    "\n",
    "# id_to_pos = np.load(\"doc_id_to_pos.npy\", mmap_mode=\"r\")\n",
    "# offsets   = np.memmap(\"titles_offsets.bin\", dtype=np.uint64, mode=\"r\")\n",
    "# data      = np.memmap(\"titles_data.bin\", dtype=np.uint8,  mode=\"r\")\n",
    "# INVALID_POS = np.uint32(2**32 - 1) if id2pos.dtype == np.uint32 else -1\n",
    "\n",
    "# def get_title(doc_id: int) -> str:\n",
    "#     pos = id_to_pos[doc_id]\n",
    "#     if pos == INVALID_POS:\n",
    "#         return \"\"\n",
    "#     start = offsets[pos]\n",
    "#     end   = offsets[pos + 1]\n",
    "#     return data[start:end].tobytes().decode(\"utf-8\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
